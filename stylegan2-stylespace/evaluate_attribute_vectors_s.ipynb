{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"]=\":16:8\"\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import sys\n",
    "import pickle\n",
    "import dnnlib\n",
    "import click\n",
    "import legacy\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from scipy.spatial.distance import cosine\n",
    "import torch.nn.functional as F\n",
    "from training.networks import SynthesisNetwork,SynthesisBlock,SynthesisLayer,ToRGBLayer\n",
    "from torch_utils import misc\n",
    "import types\n",
    "\n",
    "n_regressor_predictions = 40 # the regressor is pretrained on CelebA and predicts 40 face attributes\n",
    "device = 'cuda:0'\n",
    "batch_size = 1\n",
    "truncation_psi = 0.5\n",
    "noise_mode = 'const'\n",
    "network_pkl =  '../pretrained_models/ffhq.pkl'\n",
    "\n",
    "goal_attr_change = 2.197 # We want the sigmoid output in the range of 0.1..0.9. I\n",
    "# in case of a sigmoid output of 0.5 we only want to change the target attribute for 0.4 ~ 2.197 for the sigmoid input\n",
    "target_attr = 31\n",
    "stepsize = 0.5 \n",
    "break_delta=0.01\n",
    "max_iter=50\n",
    "\n",
    "\n",
    "def initialize_model():\n",
    "    from facenet_pytorch import MTCNN, InceptionResnetV1\n",
    "    resnet = InceptionResnetV1(pretrained='vggface2').to(device).eval()\n",
    "    return resnet\n",
    "\n",
    "face_rec = initialize_model()\n",
    "\n",
    "file_to_read = open(\"../pretrained_models/resnet_092_all_attr_5_epochs.pkl\", \"rb\")\n",
    "regressor = pickle.load(file_to_read)\n",
    "file_to_read.close()\n",
    "regressor.eval()\n",
    "regressor = regressor.to(device)\n",
    "\n",
    "def get_face_embed(img):\n",
    "    img_org_np = np.uint8(np.clip(((img.detach().cpu().numpy() + 1) / 2.0) * 255, 0, 255))\n",
    "    org = Image.fromarray(np.transpose(img_org_np[0], (1,2,0)))\n",
    "    reshaped_org = org.resize((160, 160))\n",
    "    reshaped_org = torch.Tensor(np.transpose(np.array(reshaped_org), (2,0,1))).to(device).unsqueeze(0)\n",
    "    embed_org = face_rec(reshaped_org)\n",
    "    return embed_org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def LoadModel(network_pkl,device):\n",
    "    with dnnlib.util.open_url(network_pkl) as fp:\n",
    "        G = legacy.load_network_pkl(fp)['G_ema'].requires_grad_(False).to(device) # type: ignore\n",
    "    \n",
    "    G.synthesis.forward=types.MethodType(SynthesisNetwork.forward,G.synthesis)\n",
    "    G.synthesis.W2S=types.MethodType(SynthesisNetwork.W2S,G.synthesis)\n",
    "    \n",
    "    for res in G.synthesis.block_resolutions:\n",
    "        block = getattr(G.synthesis, f'b{res}')\n",
    "        # print(block)\n",
    "        block.forward=types.MethodType(SynthesisBlock.forward,block)\n",
    "        \n",
    "        if res!=4:\n",
    "            layer=block.conv0\n",
    "            layer.forward=types.MethodType(SynthesisLayer.forward,layer)\n",
    "            layer.name='conv0_resolution_'+str(res)\n",
    "            \n",
    "        layer=block.conv1\n",
    "        layer.forward=types.MethodType(SynthesisLayer.forward,layer)\n",
    "        layer.name='conv1_resolution_'+str(res)\n",
    "        \n",
    "        layer=block.torgb\n",
    "        layer.forward=types.MethodType(ToRGBLayer.forward,layer)\n",
    "        layer.name='toRGB_resolution_'+str(res)\n",
    "        \n",
    "    \n",
    "    return G\n",
    "\n",
    "G = LoadModel(network_pkl, device)   \n",
    "label = torch.zeros([1, G.c_dim], device=device)\n",
    "\n",
    "def add_attr_vec(encoded_styles, attr_vec, epsilon):\n",
    "    new_attr_vec = {}\n",
    "    i=0\n",
    "    for key in encoded_styles.keys():\n",
    "        new_attr_vec[key] = encoded_styles[key] + attr_vec[0:, i:i+encoded_styles[key].shape[1]]*epsilon\n",
    "        i+=encoded_styles[key].shape[1]\n",
    "    return new_attr_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_attr_vec_s(l_vec, target_attr, num_samples):\n",
    "    pred_diff_list = []\n",
    "    embedding_distance_list = []\n",
    "    attr_pres_list = []\n",
    "\n",
    "    for j in range(num_samples):\n",
    "        z = torch.from_numpy(np.random.RandomState(j).randn(1, 512)).to(device)\n",
    "        w = G.mapping(z,label, truncation_psi=truncation_psi)\n",
    "        img_orig = G.synthesis(w, noise_mode=noise_mode)\n",
    "        img_orig = F.interpolate(img_orig, size=256) # reshape the image for the face_rec and regressor\n",
    "        pred_orig = regressor(img_orig).detach().cpu().numpy()\n",
    "        embedding_orig = get_face_embed(img_orig)\n",
    "        random_scaling = torch.from_numpy(np.random.RandomState(j).rand(1, 1)).to(device)\n",
    "        #random_scaling = torch.Tensor([1.0]).to(device)\n",
    "\n",
    "        if pred_orig[0, target_attr] > 0.0:\n",
    "            #img_shifted = G.synthesis(w-(l_vec*random_scaling), noise_mode=noise_mode)\n",
    "            s=G.synthesis.W2S(w)\n",
    "            s_new = add_attr_vec(s, l_vec, -random_scaling)\n",
    "            img_shifted = G.synthesis(None, encoded_styles=s_new,noise_mode='const')\n",
    "            \n",
    "        else:\n",
    "            #img_shifted = G.synthesis(w+(l_vec*random_scaling), noise_mode=noise_mode)\n",
    "            s=G.synthesis.W2S(w)\n",
    "            s_new = add_attr_vec(s, l_vec, random_scaling)\n",
    "            img_shifted = G.synthesis(None, encoded_styles=s_new,noise_mode='const')\n",
    "\n",
    "        img_shifted = F.interpolate(img_shifted, size=256) # reshape the image for the face_rec and regressor\n",
    "        pred_shifted = regressor(img_shifted).detach().cpu().numpy()\n",
    "\n",
    "        # cosine distance\n",
    "        embedding_shifted = get_face_embed(img_shifted)\n",
    "        embedding_distance = cosine(embedding_orig.detach().cpu().numpy(), embedding_shifted.detach().cpu().numpy())\n",
    "        embedding_distance_list.append(embedding_distance)\n",
    "        \n",
    "        # attribute change\n",
    "        pred_diff_list.append(np.abs(pred_shifted[0, target_attr] - pred_orig[0, target_attr] ))\n",
    "\n",
    "        # attribute preservation\n",
    "        other_pred_org = np.hstack([pred_orig[:, :int(target_attr)], pred_orig[:, int(target_attr + 1):]])\n",
    "        other_pred_shifted = np.hstack([pred_shifted[:, :int(target_attr)], pred_shifted[:, int(target_attr + 1):]])\n",
    "        attr_preservation = np.mean(np.abs(other_pred_org - other_pred_shifted))\n",
    "        attr_pres_list.append(attr_preservation)\n",
    "        \n",
    "    target_change = np.array(pred_diff_list).mean()\n",
    "    attr_dist = np.array(attr_pres_list).mean()\n",
    "    arcf_dist = np.array(embedding_distance).mean()\n",
    "    target_change_std = np.array(pred_diff_list).std()\n",
    "    attr_dist_std = np.array(attr_pres_list).std()\n",
    "    arcf_dist_std = np.array(embedding_distance_list).std()\n",
    "    return target_change, target_change_std, attr_dist, attr_dist_std, arcf_dist, arcf_dist_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pred_from_img_s(s, target_attr):\n",
    "    img = G.synthesis(None, encoded_styles=s,noise_mode='const')\n",
    "    img = F.interpolate(img, size=256) # reshape the image for the face_rec and regressor\n",
    "    pred = regressor(img).detach().cpu().numpy()\n",
    "    return pred[0, target_attr]\n",
    "\n",
    "def get_scaling_factor_s(l_vec1, goal_attr_change, target_attr, batch_size, scaling_factor, stepsize, break_delta, max_iter):\n",
    "    i = 0\n",
    "    scaling_direction_flag = 0\n",
    "\n",
    "    while(True):\n",
    "        target_attr_change_list = []\n",
    "        for j in range(batch_size):\n",
    "            z = torch.from_numpy(np.random.RandomState(j).randn(1, 512)).to(device)\n",
    "            w = G.mapping(z,label, truncation_psi=truncation_psi)\n",
    "            s=G.synthesis.W2S(w)\n",
    "            pred_orig = get_pred_from_img_s(s, target_attr)\n",
    "\n",
    "            if pred_orig > 0.0:\n",
    "                target_attr_change = get_pred_from_img_s(add_attr_vec(s, l_vec1, -scaling_factor), target_attr) - pred_orig\n",
    "            else:\n",
    "                target_attr_change = get_pred_from_img_s(add_attr_vec(s, l_vec1, scaling_factor), target_attr) - pred_orig\n",
    "\n",
    "            target_attr_change_list.append(target_attr_change)\n",
    "\n",
    "        avg_attr_delta = np.abs(np.array(target_attr_change_list)).mean()\n",
    "        \n",
    "        if np.abs(avg_attr_delta - goal_attr_change) < break_delta:\n",
    "            return scaling_factor\n",
    "        \n",
    "        else:              \n",
    "            if avg_attr_delta < goal_attr_change:\n",
    "                scaling_factor += stepsize\n",
    "                if scaling_direction_flag == -1:\n",
    "                    stepsize = stepsize/2\n",
    "                scaling_direction_flag = 1\n",
    "            elif avg_attr_delta > goal_attr_change:\n",
    "                scaling_factor -= stepsize\n",
    "                if scaling_direction_flag == 1:\n",
    "                    stepsize = stepsize/2\n",
    "                scaling_direction_flag = -1\n",
    "\n",
    "        #print(avg_attr_delta, goal_attr_change, scaling_factor)\n",
    "\n",
    "        i+=1\n",
    "        if i > max_iter:\n",
    "            return scaling_factor\n",
    "            #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_and_eval_s(attr_vec, target_attr):\n",
    "    scaling_factor = 1\n",
    "    for batch_size in [8, 32, 128, 1000]:\n",
    "        scaling_factor = get_scaling_factor_s(attr_vec, goal_attr_change, target_attr, batch_size, scaling_factor, stepsize, break_delta, max_iter)\n",
    "        print(\"bs: \", batch_size, \"scaling: \", scaling_factor)\n",
    "\n",
    "        attribute_change, attr_ch_std, attribute_preservation, attr_pres_std, similarity_metric, sim_std = eval_attr_vec_s(scaling_factor*attr_vec, target_attr, 1000)\n",
    "        print(\"attribute_change: \", attribute_change, attr_ch_std)\n",
    "        print(\"attribute_preservation: \", attribute_preservation, attr_pres_std)\n",
    "        print(\"similarity_metric: \", similarity_metric, sim_std)\n",
    "        print(\"-------------------------------\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_attribute_vec_into_row(attr_vec_path):\n",
    "    a = np.load(attr_vec_path)\n",
    "    l_vec1 = torch.zeros(1,9088)\n",
    "    z = torch.from_numpy(np.random.RandomState(0).randn(1, G.z_dim)).to(device)\n",
    "    w = G.mapping(z,label, truncation_psi=truncation_psi)\n",
    "    s = G.synthesis.W2S(w)\n",
    "\n",
    "    i=0\n",
    "    j = 0\n",
    "    for key in s.keys():\n",
    "        l_vec1[0:, i:i+s[key].shape[1]] = torch.from_numpy(a[0, j, 0:s[key].shape[1]])\n",
    "        i+=s[key].shape[1]\n",
    "        j += 1\n",
    "\n",
    "    l_vec1 = l_vec1.to(device)\n",
    "    return l_vec1\n",
    "\n",
    "print(\"l_vec_stylemc_blonde.npy\")\n",
    "attr_vec = load_attribute_vec_into_row(\"../attribute_vectors/l_vec_stylemc_blonde.npy\")\n",
    "scale_and_eval_s(attr_vec, 9)\n",
    "\n",
    "print(\"l_vec_stylemc_smile.npy\")\n",
    "attr_vec = load_attribute_vec_into_row(\"../attribute_vectors/l_vec_stylemc_smile.npy\")\n",
    "scale_and_eval_s(attr_vec, 31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"l_vec_LS-StyleEdit_hair9_s.pt\")\n",
    "attr_vec = torch.load(\"../attribute_vectors/l_vec_LS-StyleEdit_hair9_s.pt\").to(device)\n",
    "scale_and_eval_s(attr_vec, 31)\n",
    "\n",
    "print(\"l_vec_LS-StyleEdit_smile_s.pt\")\n",
    "attr_vec = torch.load(\"../attribute_vectors/l_vec_LS-StyleEdit_smile_s.pt\").to(device)\n",
    "scale_and_eval_s(attr_vec, 31)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stylegan3",
   "language": "python",
   "name": "stylegan3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
