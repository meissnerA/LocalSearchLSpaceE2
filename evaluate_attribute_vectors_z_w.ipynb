{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"]=\":16:8\"\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import sys\n",
    "stylegan2_path = './stylegan2-ada-pytorch'\n",
    "sys.path.append(stylegan2_path)\n",
    "\n",
    "import pickle\n",
    "import dnnlib\n",
    "import click\n",
    "import legacy\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from scipy.spatial.distance import cosine\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "goal_attr_change = 2.197 # We want the sigmoid output in the range of 0.1..0.9. I\n",
    "# in case of a sigmoid output of 0.5 we only want to change the target attribute for 0.4 ~ 2.197 for the sigmoid input\n",
    "target_attr = 31\n",
    "stepsize = 0.5 \n",
    "break_delta=0.01\n",
    "max_iter=50\n",
    "\n",
    "n_regressor_predictions = 40 # the regressor is pretrained on CelebA and predicts 40 face attributes\n",
    "device = 'cuda:0'\n",
    "batch_size = 1\n",
    "truncation_psi = 0.5\n",
    "noise_mode = 'const'\n",
    "network_pkl =  './pretrained_models/ffhq.pkl'\n",
    "\n",
    "with dnnlib.util.open_url(network_pkl) as f:\n",
    "    G = legacy.load_network_pkl(f)['G_ema'].to(device) # stylegan2-generator\n",
    "\n",
    "label = torch.zeros([1, G.c_dim], device=device)\n",
    "\n",
    "def initialize_model():\n",
    "    from facenet_pytorch import MTCNN, InceptionResnetV1\n",
    "    resnet = InceptionResnetV1(pretrained='vggface2').to(device).eval()\n",
    "    return resnet\n",
    "\n",
    "face_rec = initialize_model()\n",
    "\n",
    "file_to_read = open(\"./pretrained_models/resnet_092_all_attr_5_epochs.pkl\", \"rb\")\n",
    "regressor = pickle.load(file_to_read)\n",
    "file_to_read.close()\n",
    "regressor = regressor.to(device)\n",
    "regressor.eval()\n",
    "\n",
    "def get_pred_from_img(w, target_attr):\n",
    "    img = G.synthesis(w, noise_mode=noise_mode)\n",
    "    img = F.interpolate(img, size=256) # reshape the image for the face_rec and regressor\n",
    "    pred = regressor(img).detach().cpu().numpy()\n",
    "    return pred[0, target_attr]\n",
    "\n",
    "def get_face_embed(img):\n",
    "    img_org_np = np.uint8(np.clip(((img.detach().cpu().numpy() + 1) / 2.0) * 255, 0, 255))\n",
    "    org = Image.fromarray(np.transpose(img_org_np[0], (1,2,0)))\n",
    "    reshaped_org = org.resize((160, 160))\n",
    "    reshaped_org = torch.Tensor(np.transpose(np.array(reshaped_org), (2,0,1))).to(device).unsqueeze(0)\n",
    "    embed_org = face_rec(reshaped_org)\n",
    "    return embed_org"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating in Z-Space for Larsen et al."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l_vec_larsen_hair9_z.pt\n",
      "delta reached:  0.001062419891357358\n",
      "bs:  8 scaling:  0.48046875\n",
      "attribute_change:  0.6855873 0.84384847\n",
      "attribute_preservation:  0.40567508 0.3047385\n",
      "similarity_metric:  0.009771287441253662 0.005382404291572461\n",
      "-------------------------------\n",
      "delta reached:  -0.0022244186401367827\n",
      "bs:  32 scaling:  0.66015625\n",
      "attribute_change:  0.98020506 1.1740159\n",
      "attribute_preservation:  0.5348706 0.39615202\n",
      "similarity_metric:  0.013956308364868164 0.008608207461314596\n",
      "-------------------------------\n",
      "-------------------------\n",
      "l_vec_larsen_smile_z.pt\n",
      "delta reached:  -0.00976798248291022\n",
      "bs:  8 scaling:  0.6484375\n",
      "attribute_change:  1.0405023 1.017158\n",
      "attribute_preservation:  0.27644825 0.19848527\n",
      "similarity_metric:  0.007528960704803467 0.0024266278456064293\n",
      "-------------------------------\n",
      "delta reached:  0.005889442443847592\n",
      "bs:  32 scaling:  0.546875\n",
      "attribute_change:  0.86475414 0.86227775\n",
      "attribute_preservation:  0.238849 0.172537\n",
      "similarity_metric:  0.00617372989654541 0.0019528394851659142\n",
      "-------------------------------\n",
      "-------------------------\n"
     ]
    }
   ],
   "source": [
    "def get_scaling_factor_z(l_vec1, goal_attr_change, target_attr, batch_size, scaling_factor, stepsize, break_delta, max_iter):\n",
    "    i = 0\n",
    "    scaling_direction_flag = 0\n",
    "\n",
    "    while(True):\n",
    "        target_attr_change_list = []\n",
    "        for j in range(batch_size):\n",
    "            z = torch.from_numpy(np.random.RandomState(j).randn(1, 512)).to(device)\n",
    "            w = G.mapping(z,label, truncation_psi=truncation_psi)\n",
    "            pred_orig = get_pred_from_img(w, target_attr)\n",
    "            \n",
    "            if pred_orig > 0.0:\n",
    "                z_new = z - (l_vec1*scaling_factor) # differs here compared to w-Space\n",
    "                w_new = G.mapping(z_new,label, truncation_psi=truncation_psi)\n",
    "                target_attr_change = get_pred_from_img(w_new, target_attr) - pred_orig\n",
    "            else:\n",
    "                z_new = z + (l_vec1*scaling_factor) # differs here compared to w-Space\n",
    "                w_new = G.mapping(z_new,label, truncation_psi=truncation_psi)\n",
    "                target_attr_change = get_pred_from_img(w_new, target_attr) - pred_orig\n",
    "\n",
    "            target_attr_change_list.append(target_attr_change)\n",
    "\n",
    "        avg_attr_delta = np.abs(np.array(target_attr_change_list)).mean()\n",
    "\n",
    "        if np.abs(avg_attr_delta - goal_attr_change) < break_delta:\n",
    "            print(\"delta reached: \", avg_attr_delta - goal_attr_change)\n",
    "            return scaling_factor\n",
    "        else:              \n",
    "            if avg_attr_delta < goal_attr_change:\n",
    "                scaling_factor += stepsize\n",
    "                if scaling_direction_flag == -1:\n",
    "                    stepsize = stepsize/2\n",
    "                scaling_direction_flag = 1\n",
    "            elif avg_attr_delta > goal_attr_change:\n",
    "                scaling_factor -= stepsize\n",
    "                if scaling_direction_flag == 1:\n",
    "                    stepsize = stepsize/2\n",
    "                scaling_direction_flag = -1\n",
    "\n",
    "        i+=1\n",
    "        if i > max_iter:\n",
    "            return scaling_factor\n",
    "\n",
    "\n",
    "def eval_attr_vec_z(l_vec, target_attr, num_samples):\n",
    "    pred_diff_list = []\n",
    "    embedding_distance_list = []\n",
    "    attr_pres_list = []\n",
    "\n",
    "    for j in range(num_samples):\n",
    "        z = torch.from_numpy(np.random.RandomState(j).randn(1, 512)).to(device)\n",
    "        w = G.mapping(z,label, truncation_psi=truncation_psi)\n",
    "        img_orig = G.synthesis(w, noise_mode=noise_mode)\n",
    "        img_orig = F.interpolate(img_orig, size=256) # reshape the image for the face_rec and regressor\n",
    "        pred_orig = regressor(img_orig).detach().cpu().numpy()\n",
    "        embedding_orig = get_face_embed(img_orig)\n",
    "        random_scaling = torch.from_numpy(np.random.RandomState(j).rand(1, 1)).to(device) \n",
    "\n",
    "        if pred_orig[0, target_attr] > 0.0:\n",
    "            z_new = z - (l_vec*random_scaling) # differs here compared to w-Space\n",
    "            w_new = G.mapping(z_new,label, truncation_psi=truncation_psi)\n",
    "            img_shifted = G.synthesis(w_new, noise_mode=noise_mode)\n",
    "            \n",
    "        else:\n",
    "            z_new = z + (l_vec*random_scaling) # differs here compared to w-Space\n",
    "            w_new = G.mapping(z_new,label, truncation_psi=truncation_psi)\n",
    "            img_shifted = G.synthesis(w_new, noise_mode=noise_mode)\n",
    "\n",
    "        img_shifted = F.interpolate(img_shifted, size=256) # reshape the image for the face_rec and regressor\n",
    "        pred_shifted = regressor(img_shifted).detach().cpu().numpy()\n",
    "\n",
    "        # cosine distance\n",
    "        embedding_shifted = get_face_embed(img_shifted)\n",
    "        embedding_distance = cosine(embedding_orig.detach().cpu().numpy(), embedding_shifted.detach().cpu().numpy())\n",
    "        embedding_distance_list.append(embedding_distance)\n",
    "        \n",
    "        # attribute change\n",
    "        pred_diff_list.append(np.abs(pred_shifted[0, target_attr] - pred_orig[0, target_attr] ))\n",
    "\n",
    "        # attribute preservation\n",
    "        other_pred_org = np.hstack([pred_orig[:, :int(target_attr)], pred_orig[:, int(target_attr + 1):]])\n",
    "        other_pred_shifted = np.hstack([pred_shifted[:, :int(target_attr)], pred_shifted[:, int(target_attr + 1):]])\n",
    "        attr_preservation = np.mean(np.abs(other_pred_org - other_pred_shifted))\n",
    "        attr_pres_list.append(attr_preservation)\n",
    "        \n",
    "    target_change = np.array(pred_diff_list).mean()\n",
    "    attr_dist = np.array(attr_pres_list).mean()\n",
    "    arcf_dist = np.array(embedding_distance).mean()\n",
    "    target_change_std = np.array(pred_diff_list).std()\n",
    "    attr_dist_std = np.array(attr_pres_list).std()\n",
    "    arcf_dist_std = np.array(embedding_distance_list).std()\n",
    "    return target_change, target_change_std, attr_dist, attr_dist_std, arcf_dist, arcf_dist_std\n",
    "    \n",
    "\n",
    "def scale_and_eval_z(attr_vec, target_attr):\n",
    "    scaling_factor = 1\n",
    "    for batch_size in [8, 32]: # habe die 1000 für einen speedup entfernt\n",
    "        scaling_factor = get_scaling_factor_z(attr_vec, goal_attr_change, target_attr, batch_size, scaling_factor, stepsize, break_delta, max_iter)\n",
    "        print(\"bs: \", batch_size, \"scaling: \", scaling_factor)\n",
    "\n",
    "        attribute_change, attr_ch_std, attribute_preservation, attr_pres_std, similarity_metric, sim_std = eval_attr_vec_z(scaling_factor*attr_vec, target_attr, 1000)\n",
    "        print(\"attribute_change: \", attribute_change, attr_ch_std)\n",
    "        print(\"attribute_preservation: \", attribute_preservation, attr_pres_std)\n",
    "        print(\"similarity_metric: \", similarity_metric, sim_std)\n",
    "        print(\"-------------------------------\")\n",
    "\n",
    "l_vec_smile_z = torch.load(\"./attribute_vectors/l_vec_larsen_smile_z.pt\", map_location=device)\n",
    "l_vec_hair9_z = torch.load(\"./attribute_vectors/l_vec_larsen_hair9_z.pt\", map_location=device)\n",
    "\n",
    "print(\"l_vec_larsen_hair9_z.pt\")\n",
    "scale_and_eval_z(l_vec_hair9_z, 9) # 9 is haircolor\n",
    "print(\"-------------------------\")\n",
    "\n",
    "print(\"l_vec_larsen_smile_z.pt\")\n",
    "scale_and_eval_z(l_vec_smile_z, 31) # 31 is smile\n",
    "print(\"-------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating in W-Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scaling_factor(l_vec1, goal_attr_change, target_attr, batch_size, scaling_factor, stepsize, break_delta, max_iter):\n",
    "    i = 0\n",
    "    scaling_direction_flag = 0\n",
    "\n",
    "    while(True):\n",
    "        target_attr_change_list = []\n",
    "        for j in range(batch_size):\n",
    "            z = torch.from_numpy(np.random.RandomState(j).randn(1, 512)).to(device)\n",
    "            w = G.mapping(z,label, truncation_psi=truncation_psi)\n",
    "            pred_orig = get_pred_from_img(w, target_attr)\n",
    "\n",
    "            if pred_orig > 0.0:\n",
    "                target_attr_change = get_pred_from_img(w-(l_vec1*scaling_factor), target_attr) - pred_orig\n",
    "            else:\n",
    "                target_attr_change = get_pred_from_img(w+(l_vec1*scaling_factor), target_attr) - pred_orig\n",
    "\n",
    "            target_attr_change_list.append(target_attr_change)\n",
    "\n",
    "        avg_attr_delta = np.abs(np.array(target_attr_change_list)).mean()\n",
    "\n",
    "        if np.abs(avg_attr_delta - goal_attr_change) < break_delta:\n",
    "            print(\"delta reached: \", avg_attr_delta - goal_attr_change)\n",
    "            return scaling_factor\n",
    "            \n",
    "        else:              \n",
    "            if avg_attr_delta < goal_attr_change:\n",
    "                scaling_factor += stepsize\n",
    "                if scaling_direction_flag == -1:\n",
    "                    stepsize = stepsize/2\n",
    "                scaling_direction_flag = 1\n",
    "            elif avg_attr_delta > goal_attr_change:\n",
    "                scaling_factor -= stepsize\n",
    "                if scaling_direction_flag == 1:\n",
    "                    stepsize = stepsize/2\n",
    "                scaling_direction_flag = -1\n",
    "\n",
    "        i+=1\n",
    "        if i > max_iter:\n",
    "            return scaling_factor\n",
    "            #break\n",
    "\n",
    "\n",
    "def eval_attr_vec(l_vec, target_attr, num_samples):\n",
    "    pred_diff_list = []\n",
    "    embedding_distance_list = []\n",
    "    attr_pres_list = []\n",
    "\n",
    "    for j in range(num_samples):\n",
    "        z = torch.from_numpy(np.random.RandomState(j).randn(1, 512)).to(device)\n",
    "        w = G.mapping(z,label, truncation_psi=truncation_psi)\n",
    "        img_orig = G.synthesis(w, noise_mode=noise_mode)\n",
    "        img_orig = F.interpolate(img_orig, size=256) # reshape the image for the face_rec and regressor\n",
    "        pred_orig = regressor(img_orig).detach().cpu().numpy()\n",
    "        embedding_orig = get_face_embed(img_orig)\n",
    "        random_scaling = torch.from_numpy(np.random.RandomState(j).rand(1, 1)).to(device) # ÄNDERUNG fürs scaling!!!\n",
    "\n",
    "        if pred_orig[0, target_attr] > 0.0:\n",
    "            img_shifted = G.synthesis(w-(l_vec*random_scaling), noise_mode=noise_mode)\n",
    "        else:\n",
    "            img_shifted = G.synthesis(w+(l_vec*random_scaling), noise_mode=noise_mode)\n",
    "\n",
    "        img_shifted = F.interpolate(img_shifted, size=256) # reshape the image for the face_rec and regressor\n",
    "        pred_shifted = regressor(img_shifted).detach().cpu().numpy()\n",
    "\n",
    "        # cosine distance\n",
    "        embedding_shifted = get_face_embed(img_shifted)\n",
    "        embedding_distance = cosine(embedding_orig.detach().cpu().numpy(), embedding_shifted.detach().cpu().numpy())\n",
    "        embedding_distance_list.append(embedding_distance)\n",
    "        \n",
    "        # attribute change\n",
    "        pred_diff_list.append(np.abs(pred_shifted[0, target_attr] - pred_orig[0, target_attr] ))\n",
    "\n",
    "        # attribute preservation\n",
    "        other_pred_org = np.hstack([pred_orig[:, :int(target_attr)], pred_orig[:, int(target_attr + 1):]])\n",
    "        other_pred_shifted = np.hstack([pred_shifted[:, :int(target_attr)], pred_shifted[:, int(target_attr + 1):]])\n",
    "        attr_preservation = np.mean(np.abs(other_pred_org - other_pred_shifted))\n",
    "        attr_pres_list.append(attr_preservation)\n",
    "        \n",
    "    target_change = np.array(pred_diff_list).mean()\n",
    "    attr_dist = np.array(attr_pres_list).mean()\n",
    "    arcf_dist = np.array(embedding_distance).mean()\n",
    "    target_change_std = np.array(pred_diff_list).std()\n",
    "    attr_dist_std = np.array(attr_pres_list).std()\n",
    "    arcf_dist_std = np.array(embedding_distance_list).std()\n",
    "    return target_change, target_change_std, attr_dist, attr_dist_std, arcf_dist, arcf_dist_std\n",
    "\n",
    "\n",
    "\n",
    "goal_attr_change = 2.197 # in a case of an initial attribute vector of 0.5 this changes the sigmoid value by 0.4\n",
    "target_attr = 31\n",
    "stepsize = 0.5 \n",
    "break_delta=0.01\n",
    "max_iter=50\n",
    "\n",
    "def scale_and_eval(attr_vec, target_attr):\n",
    "    scaling_factor = 1.0\n",
    "    for batch_size in [8, 32]: \n",
    "        scaling_factor = get_scaling_factor(attr_vec, goal_attr_change, target_attr, batch_size, scaling_factor, stepsize, break_delta, max_iter)\n",
    "        print(\"bs: \", batch_size, \"scaling: \", scaling_factor)\n",
    "\n",
    "        attribute_change, attr_change_std, attribute_preservation, attr_pres_std, similarity_metric, sim_metric_std = eval_attr_vec(scaling_factor*attr_vec, target_attr, batch_size)\n",
    "        print(\"attribute_change: \", attribute_change, attr_change_std)\n",
    "        print(\"attribute_preservation: \", attribute_preservation, attr_pres_std)\n",
    "        print(\"similarity_metric: \", similarity_metric, sim_metric_std)\n",
    "\n",
    "        print(\"-------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l_vec_larsen_hair9_w\n",
      "delta reached:  -0.0025624961853027983\n",
      "bs:  8 scaling:  0.375\n",
      "attribute_change:  1.1518537 1.1955967\n",
      "attribute_preservation:  0.5238164 0.42857862\n",
      "similarity_metric:  9.191036224365234e-05 0.00352196014370181\n",
      "-------------------------------\n",
      "delta reached:  -0.004579517364502017\n",
      "bs:  32 scaling:  0.44921875\n",
      "attribute_change:  0.8327917 0.8929268\n",
      "attribute_preservation:  0.52067816 0.3828484\n",
      "similarity_metric:  0.0010453462600708008 0.004496470586099009\n",
      "-------------------------------\n",
      "l_vec_larsen_smile_w\n",
      "delta reached:  -0.00984284591674811\n",
      "bs:  8 scaling:  0.30078125\n",
      "attribute_change:  1.2729964 0.99157846\n",
      "attribute_preservation:  0.28808972 0.16952616\n",
      "similarity_metric:  6.9141387939453125e-06 0.0007238071992392287\n",
      "-------------------------------\n",
      "delta reached:  0.009894874572753842\n",
      "bs:  32 scaling:  0.2890625\n",
      "attribute_change:  1.1427507 1.0444833\n",
      "attribute_preservation:  0.21359012 0.14280128\n",
      "similarity_metric:  4.6372413635253906e-05 0.0005941820974235821\n",
      "-------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"l_vec_larsen_hair9_w\")\n",
    "attr_vec = torch.load(\"./attribute_vectors/l_vec_larsen_hair9_w.pt\", map_location=device)\n",
    "scale_and_eval(attr_vec, 9) \n",
    "\n",
    "print(\"l_vec_larsen_smile_w\")\n",
    "attr_vec = torch.load(\"./attribute_vectors/l_vec_larsen_smile_w.pt\", map_location=device)\n",
    "scale_and_eval(attr_vec, 31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l_vec_shen_hair9_w.npy\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "from_numpy() takes no keyword arguments",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_27017/3076002921.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"l_vec_shen_hair9_w.npy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mattr_vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./attribute_vectors/l_vec_shen_hair9_w.npy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mscale_and_eval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattr_vec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"shen_smiling_w.npy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: from_numpy() takes no keyword arguments"
     ]
    }
   ],
   "source": [
    "print(\"l_vec_shen_hair9_w.npy\")\n",
    "attr_vec = torch.from_numpy(np.load(\"./attribute_vectors/l_vec_shen_hair9_w.npy\")).to(device)\n",
    "scale_and_eval(attr_vec, 9)\n",
    "\n",
    "print(\"shen_smiling_w.npy\")\n",
    "attr_vec = torch.from_numpy(np.load(\"./attribute_vectors/l_vec_shen_smile_w.npy\")).to(device)\n",
    "scale_and_eval(attr_vec, 31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zhuang_hair_col_9_w\n",
      "delta reached:  -0.004005882263183658\n",
      "bs:  8 scaling:  0.201171875\n",
      "attribute_change:  1.1513443 1.145913\n",
      "attribute_preservation:  0.20349863 0.14837822\n",
      "similarity_metric:  5.620718002319336e-05 0.004495562677234512\n",
      "-------------------------------\n",
      "delta reached:  -0.0001530380249024077\n",
      "bs:  32 scaling:  0.248046875\n",
      "attribute_change:  0.8933507 0.8757463\n",
      "attribute_preservation:  0.2437277 0.15271558\n",
      "similarity_metric:  0.0005311369895935059 0.004209498371201054\n",
      "-------------------------------\n",
      "zhuang_smiling_w\n",
      "delta reached:  -0.0051264495849610014\n",
      "bs:  8 scaling:  0.189453125\n",
      "attribute_change:  1.246439 0.964671\n",
      "attribute_preservation:  0.24842453 0.15758057\n",
      "similarity_metric:  3.874301910400391e-06 0.0006259568222167492\n",
      "-------------------------------\n",
      "bs:  32 scaling:  -25.310546875\n",
      "attribute_change:  4.58639 2.7494304\n",
      "attribute_preservation:  2.8290644 1.0207511\n",
      "similarity_metric:  0.04728376865386963 0.05113052201497457\n",
      "-------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"zhuang_hair_col_9_w\")\n",
    "attr_vec = torch.load(\"./attribute_vectors/l_vec_zhuang_hair9_w.pt\", map_location=device)\n",
    "scale_and_eval(attr_vec, 9)\n",
    "\n",
    "print(\"zhuang_smiling_w\")\n",
    "attr_vec = torch.load(\"./attribute_vectors/l_vec_zhuang_smile_w.pt\", map_location=device)\n",
    "scale_and_eval(attr_vec, 31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l_vec_LS-StyleEdit_hair9_w\n",
      "delta reached:  0.0017919807434081392\n",
      "bs:  8 scaling:  0.1220703125\n",
      "attribute_change:  1.1609406 1.1405779\n",
      "attribute_preservation:  0.253304 0.13630417\n",
      "similarity_metric:  4.935264587402344e-05 0.005647686298331719\n",
      "-------------------------------\n",
      "delta reached:  0.007776287078857358\n",
      "bs:  32 scaling:  0.1611328125\n",
      "attribute_change:  0.8941095 0.917514\n",
      "attribute_preservation:  0.30026308 0.17880459\n",
      "similarity_metric:  0.0005733370780944824 0.005830954863870075\n",
      "-------------------------------\n",
      "l_vec_LS-StyleEdit_smile_w\n",
      "delta reached:  -0.007851812362670962\n",
      "bs:  8 scaling:  0.34765625\n",
      "attribute_change:  1.2244022 0.9263094\n",
      "attribute_preservation:  0.24829997 0.1602671\n",
      "similarity_metric:  3.814697265625e-06 0.0005230662642874221\n",
      "-------------------------------\n",
      "delta reached:  -0.009271595001220767\n",
      "bs:  32 scaling:  0.32421875\n",
      "attribute_change:  1.1481775 1.0420691\n",
      "attribute_preservation:  0.18176383 0.13103719\n",
      "similarity_metric:  1.9609928131103516e-05 0.00029666699941712795\n",
      "-------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"l_vec_LS-StyleEdit_hair9_w\")\n",
    "attr_vec = torch.load(\"./attribute_vectors/l_vec_LS-StyleEdit_hair9_w.pt\", map_location=device)\n",
    "scale_and_eval(attr_vec, 9)\n",
    "\n",
    "print(\"l_vec_LS-StyleEdit_smile_w\")\n",
    "attr_vec = torch.load(\"./attribute_vectors/l_vec_LS-StyleEdit_smile_w.pt\", map_location=device)\n",
    "scale_and_eval(attr_vec, 31)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating in S-Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uninstalling torch-utils-0.1.2:\n",
      "  Would remove:\n",
      "    /home/hermiod/projects/tests/stylegan3/venv_faceon/lib/python3.8/site-packages/torch_utils-0.1.2.dist-info/*\n",
      "    /home/hermiod/projects/tests/stylegan3/venv_faceon/lib/python3.8/site-packages/torch_utils/*\n",
      "Proceed (y/n)? ^C\n",
      "\u001b[31mERROR: Operation cancelled by user\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip uninstall torch-utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os._exit(00)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'misc' from 'torch_utils' (/home/hermiod/projects/tests/stylegan3/venv_faceon/lib/python3.8/site-packages/torch_utils/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_29266/3190392941.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspatial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistance\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcosine\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtmp_styleclip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetworks\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSynthesisNetwork\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mSynthesisBlock\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mSynthesisLayer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mToRGBLayer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtmp_styleclip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtorch_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmisc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tmp/LocalSearchLSpaceE2/tmp_styleclip/training/networks.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmisc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpersistence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconv2d_resample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'misc' from 'torch_utils' (/home/hermiod/projects/tests/stylegan3/venv_faceon/lib/python3.8/site-packages/torch_utils/__init__.py)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"]=\":16:8\"\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from natsort import natsorted\n",
    "import sys\n",
    "library_path = os.path.abspath('./tmp_styleclip')\n",
    "\n",
    "# Add the library path to sys.path\n",
    "sys.path.insert(0, library_path)\n",
    "\n",
    "import pickle\n",
    "import dnnlib\n",
    "import click\n",
    "from tmp_styleclip import legacy\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from scipy.spatial.distance import cosine\n",
    "import torch.nn.functional as F\n",
    "from tmp_styleclip.training.networks import SynthesisNetwork,SynthesisBlock,SynthesisLayer,ToRGBLayer\n",
    "from tmp_styleclip.torch_utils import misc\n",
    "import types\n",
    "\n",
    "n_regressor_predictions = 40 # the regressor is pretrained on CelebA and predicts 40 face attributes\n",
    "device = 'cuda:0'\n",
    "batch_size = 1\n",
    "truncation_psi = 0.5\n",
    "noise_mode = 'const'\n",
    "network_pkl =  '../../pretrained_models/ffhq.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'misc' from 'torch_utils' (/home/hermiod/projects/tests/stylegan3/venv_faceon/lib/python3.8/site-packages/torch_utils/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_29266/957002310.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtmp_styleclip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetworks\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSynthesisNetwork\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mSynthesisBlock\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mSynthesisLayer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mToRGBLayer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mLoadModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnetwork_pkl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mdnnlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnetwork_pkl\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tmp/LocalSearchLSpaceE2/tmp_styleclip/training/networks.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmisc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpersistence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconv2d_resample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'misc' from 'torch_utils' (/home/hermiod/projects/tests/stylegan3/venv_faceon/lib/python3.8/site-packages/torch_utils/__init__.py)"
     ]
    }
   ],
   "source": [
    "import types\n",
    "from tmp_styleclip.training.networks import SynthesisNetwork,SynthesisBlock,SynthesisLayer,ToRGBLayer\n",
    "\n",
    "def LoadModel(network_pkl,device):\n",
    "    with dnnlib.util.open_url(network_pkl) as fp:\n",
    "        G = legacy.load_network_pkl(fp)['G_ema'].requires_grad_(False).to(device) # type: ignore\n",
    "    \n",
    "    G.synthesis.forward=types.MethodType(SynthesisNetwork.forward,G.synthesis)\n",
    "    G.synthesis.W2S=types.MethodType(SynthesisNetwork.W2S,G.synthesis)\n",
    "    \n",
    "    for res in G.synthesis.block_resolutions:\n",
    "        block = getattr(G.synthesis, f'b{res}')\n",
    "        # print(block)\n",
    "        block.forward=types.MethodType(SynthesisBlock.forward,block)\n",
    "        \n",
    "        if res!=4:\n",
    "            layer=block.conv0\n",
    "            layer.forward=types.MethodType(SynthesisLayer.forward,layer)\n",
    "            layer.name='conv0_resolution_'+str(res)\n",
    "            \n",
    "        layer=block.conv1\n",
    "        layer.forward=types.MethodType(SynthesisLayer.forward,layer)\n",
    "        layer.name='conv1_resolution_'+str(res)\n",
    "        \n",
    "        layer=block.torgb\n",
    "        layer.forward=types.MethodType(ToRGBLayer.forward,layer)\n",
    "        layer.name='toRGB_resolution_'+str(res)\n",
    "        \n",
    "    \n",
    "    return G\n",
    "\n",
    "G = LoadModel(network_pkl, device)   \n",
    "label = torch.zeros([1, G.c_dim], device=device)\n",
    "\n",
    "def add_attr_vec(encoded_styles, attr_vec, epsilon):\n",
    "    new_attr_vec = {}\n",
    "    i=0\n",
    "    for key in encoded_styles.keys():\n",
    "        new_attr_vec[key] = encoded_styles[key] + attr_vec[0:, i:i+encoded_styles[key].shape[1]]*epsilon\n",
    "        i+=encoded_styles[key].shape[1]\n",
    "    return new_attr_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_attr_vec_s(l_vec, target_attr, num_samples):\n",
    "    pred_diff_list = []\n",
    "    embedding_distance_list = []\n",
    "    attr_pres_list = []\n",
    "\n",
    "    for j in range(num_samples):\n",
    "        z = torch.from_numpy(np.random.RandomState(j).randn(1, 512)).to(device)\n",
    "        w = G.mapping(z,label, truncation_psi=truncation_psi)\n",
    "        img_orig = G.synthesis(w, noise_mode=noise_mode)\n",
    "        img_orig = F.interpolate(img_orig, size=256) # reshape the image for the face_rec and regressor\n",
    "        pred_orig = regressor(img_orig).detach().cpu().numpy()\n",
    "        embedding_orig = get_face_embed(img_orig)\n",
    "        random_scaling = torch.from_numpy(np.random.RandomState(j).rand(1, 1)).to(device)\n",
    "        #random_scaling = torch.Tensor([1.0]).to(device)\n",
    "\n",
    "        if pred_orig[0, target_attr] > 0.0:\n",
    "            #img_shifted = G.synthesis(w-(l_vec*random_scaling), noise_mode=noise_mode)\n",
    "            s=G.synthesis.W2S(w)\n",
    "            s_new = add_attr_vec(s, l_vec, -random_scaling)\n",
    "            img_shifted = G.synthesis(None, encoded_styles=s_new,noise_mode='const')\n",
    "            \n",
    "        else:\n",
    "            #img_shifted = G.synthesis(w+(l_vec*random_scaling), noise_mode=noise_mode)\n",
    "            s=G.synthesis.W2S(w)\n",
    "            s_new = add_attr_vec(s, l_vec, random_scaling)\n",
    "            img_shifted = G.synthesis(None, encoded_styles=s_new,noise_mode='const')\n",
    "\n",
    "        img_shifted = F.interpolate(img_shifted, size=256) # reshape the image for the face_rec and regressor\n",
    "        pred_shifted = regressor(img_shifted).detach().cpu().numpy()\n",
    "\n",
    "        # cosine distance\n",
    "        embedding_shifted = get_face_embed(img_shifted)\n",
    "        embedding_distance = cosine(embedding_orig.detach().cpu().numpy(), embedding_shifted.detach().cpu().numpy())\n",
    "        embedding_distance_list.append(embedding_distance)\n",
    "        \n",
    "        # attribute change\n",
    "        pred_diff_list.append(np.abs(pred_shifted[0, target_attr] - pred_orig[0, target_attr] ))\n",
    "\n",
    "        # attribute preservation\n",
    "        other_pred_org = np.hstack([pred_orig[:, :int(target_attr)], pred_orig[:, int(target_attr + 1):]])\n",
    "        other_pred_shifted = np.hstack([pred_shifted[:, :int(target_attr)], pred_shifted[:, int(target_attr + 1):]])\n",
    "        attr_preservation = np.mean(np.abs(other_pred_org - other_pred_shifted))\n",
    "        attr_pres_list.append(attr_preservation)\n",
    "        \n",
    "    target_change = np.array(pred_diff_list).mean()\n",
    "    attr_dist = np.array(attr_pres_list).mean()\n",
    "    arcf_dist = np.array(embedding_distance).mean()\n",
    "    target_change_std = np.array(pred_diff_list).std()\n",
    "    attr_dist_std = np.array(attr_pres_list).std()\n",
    "    arcf_dist_std = np.array(embedding_distance_list).std()\n",
    "    return target_change, target_change_std, attr_dist, attr_dist_std, arcf_dist, arcf_dist_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pred_from_img_s(s, target_attr):\n",
    "    img = G.synthesis(None, encoded_styles=s,noise_mode='const')\n",
    "    img = F.interpolate(img, size=256) # reshape the image for the face_rec and regressor\n",
    "    pred = regressor(img).detach().cpu().numpy()\n",
    "    return pred[0, target_attr]\n",
    "\n",
    "def get_scaling_factor_s(l_vec1, goal_attr_change, target_attr, batch_size, scaling_factor, stepsize, break_delta, max_iter):\n",
    "    i = 0\n",
    "    scaling_direction_flag = 0\n",
    "\n",
    "    while(True):\n",
    "        target_attr_change_list = []\n",
    "        for j in range(batch_size):\n",
    "            z = torch.from_numpy(np.random.RandomState(j).randn(1, 512)).to(device)\n",
    "            w = G.mapping(z,label, truncation_psi=truncation_psi)\n",
    "            s=G.synthesis.W2S(w)\n",
    "            pred_orig = get_pred_from_img_s(s, target_attr)\n",
    "\n",
    "            if pred_orig > 0.0:\n",
    "                target_attr_change = get_pred_from_img_s(add_attr_vec(s, l_vec1, -scaling_factor), target_attr) - pred_orig\n",
    "            else:\n",
    "                target_attr_change = get_pred_from_img_s(add_attr_vec(s, l_vec1, scaling_factor), target_attr) - pred_orig\n",
    "\n",
    "            target_attr_change_list.append(target_attr_change)\n",
    "\n",
    "        avg_attr_delta = np.abs(np.array(target_attr_change_list)).mean()\n",
    "        \n",
    "        if np.abs(avg_attr_delta - goal_attr_change) < break_delta:\n",
    "            #print(\"delta reached: \", avg_attr_delta - goal_attr_change)\n",
    "            return scaling_factor, avg_attr_delta\n",
    "            #break\n",
    "        else:              \n",
    "            if avg_attr_delta < goal_attr_change:\n",
    "                scaling_factor += stepsize\n",
    "                #print(\"positive\", avg_attr_delta, goal_attr_change)\n",
    "                if scaling_direction_flag == -1:\n",
    "                    stepsize = stepsize/2\n",
    "                scaling_direction_flag = 1\n",
    "            elif avg_attr_delta > goal_attr_change:\n",
    "                scaling_factor -= stepsize\n",
    "                #print(\"negative\")\n",
    "                if scaling_direction_flag == 1:\n",
    "                    stepsize = stepsize/2\n",
    "                scaling_direction_flag = -1\n",
    "\n",
    "        #print(avg_attr_delta, goal_attr_change, scaling_factor)\n",
    "\n",
    "        i+=1\n",
    "        if i > max_iter:\n",
    "            return scaling_factor, avg_attr_delta\n",
    "            #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l_vec_stylemc_blonde.npy\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (256) must match the size of tensor b (512) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_27017/3513917441.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"l_vec_stylemc_blonde.npy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mattr_vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./attribute_vectors/l_vec_stylemc_blonde.npy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mscale_and_eval_s\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattr_vec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"l_vec_stylemc_smile.npy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_27017/3513917441.py\u001b[0m in \u001b[0;36mscale_and_eval_s\u001b[0;34m(attr_vec, target_attr)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mscaling_factor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# habe die 1000 für einen speedup entfernt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mscaling_factor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_scaling_factor_s\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattr_vec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgoal_attr_change\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_attr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaling_factor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstepsize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbreak_delta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"bs: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"scaling: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaling_factor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_27017/2948314010.py\u001b[0m in \u001b[0;36mget_scaling_factor_s\u001b[0;34m(l_vec1, goal_attr_change, target_attr, batch_size, scaling_factor, stepsize, break_delta, max_iter)\u001b[0m\n\u001b[1;32m     20\u001b[0m                 \u001b[0mtarget_attr_change\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_pred_from_img_s\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madd_attr_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml_vec1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mscaling_factor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_attr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mpred_orig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m                 \u001b[0mtarget_attr_change\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_pred_from_img_s\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madd_attr_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml_vec1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaling_factor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_attr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mpred_orig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0mtarget_attr_change_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_attr_change\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_27017/83674298.py\u001b[0m in \u001b[0;36madd_attr_vec\u001b[0;34m(encoded_styles, attr_vec, epsilon)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0mi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mencoded_styles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0mnew_attr_vec\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoded_styles\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mattr_vec\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mencoded_styles\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0mi\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0mencoded_styles\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnew_attr_vec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (256) must match the size of tensor b (512) at non-singleton dimension 2"
     ]
    }
   ],
   "source": [
    "def scale_and_eval_s(attr_vec, target_attr):\n",
    "    scaling_factor = 1\n",
    "    for batch_size in [8, 32]: # habe die 1000 für einen speedup entfernt\n",
    "        scaling_factor = get_scaling_factor_s(attr_vec, goal_attr_change, target_attr, batch_size, scaling_factor, stepsize, break_delta, max_iter)\n",
    "        print(\"bs: \", batch_size, \"scaling: \", scaling_factor)\n",
    "\n",
    "        attribute_change, attr_ch_std, attribute_preservation, attr_pres_std, similarity_metric, sim_std = eval_attr_vec_z(scaling_factor*attr_vec, target_attr, 1000)\n",
    "        print(\"attribute_change: \", attribute_change, attr_ch_std)\n",
    "        print(\"attribute_preservation: \", attribute_preservation, attr_pres_std)\n",
    "        print(\"similarity_metric: \", similarity_metric, sim_std)\n",
    "        print(\"-------------------------------\")\n",
    "\n",
    "print(\"l_vec_stylemc_blonde.npy\")\n",
    "attr_vec = torch.from_numpy(np.load(\"./attribute_vectors/l_vec_stylemc_blonde.npy\")).to(device)\n",
    "scale_and_eval_s(attr_vec, 9)\n",
    "\n",
    "print(\"l_vec_stylemc_smile.npy\")\n",
    "attr_vec = torch.from_numpy(np.load(\"./attribute_vectors/l_vec_stylemc_smile.npy\")).to(device)\n",
    "scale_and_eval_s(attr_vec, 31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"l_vec_LS-StyleEdit_hair9_w\")\n",
    "attr_vec = torch.load(\"./attribute_vectors/l_vec_LS-StyleEdit_hair9_s.pt\", map_location=device)\n",
    "scale_and_eval_s(attr_vec, 9)\n",
    "\n",
    "print(\"l_vec_LS-StyleEdit_smile_w\")\n",
    "attr_vec = torch.load(\"./attribute_vectors/l_vec_LS-StyleEdit_smile_s.pt\", map_location=device)\n",
    "scale_and_eval_s(attr_vec, 31)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stylegan3",
   "language": "python",
   "name": "stylegan3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
